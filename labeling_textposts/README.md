# Labeling text posts
Trying to create cohesive categorizations for anonymous textposts
***
## Part 1: Goal
My new assignment works with both supervised and unsupervised learning tasks. I also worked iteratively and reflectively to apply machine learning techniques to a data set of interest with informative documentation, written for a variety of audiences. I also detail some ethical implications. My project uses a dataset of posts taken from the Smith Confessional, and anonymous posting forum used by Smith students. There are a lot of different topics and discussions on the Confessional, and thousands of posts, so I wanted to create a machine learning algorithm that could look at the text of posts and organize them into categories depending on their characteristics.  


## Part 2: Unsupervised Machine Learning

This project begins with unsupervised tasks, because I had not labeled any of my data so it is up to the model to draw patterns from my data. I felt unsupervised machine learning was the best way to go about labeling this kind of data for ethical reasons. A lot of Confessional posts are very emotionally charged and tend to have strong unspoken connotations associated with them. Being only one person, I felt uncomfortable personally grouping them, due to my own implicit biases and inability to accurately assess the tone and intent of the original author, due to the anonymity of the site. So for my initial, exploratory analysis, I felt it was more ethical to give unsupervised machine learning a show to create the initial data categories, and then refine the labels based on human comprehension. 

There was also a practical aspect to this strategy. There are way too many posts for me to reasonably go through and make holistic categories on my own. Therefore, it would be extremely beneficial for me to use machine learning to at leaststart the process of categorization. 

I read my data in from the `Confesh_posts_master.csv` file. I then used sklearn's `TfidfVectorizer` to tokenize my posts. The `TfidfVectorizer` function pre processed my data so that I could avoid weird errors from unrecognized characters (like emojis) and that only important words are counted. In the parameters of this function, I set `TfidfVectorizer` to remove white space and punctuation from the posts, remove English stopwords which are common but do not add any information to my analysis (like "the"), filter out words that have very few occurances (appear in less than .01% of the posts), and remove special characters that could confuse my later functions. Once the text data was fitered, `TfidfVectorizer` tokenizes the words that made it through the filtering process (assigns them numerical identifiers) and counts how many times they occur. I used `TfidfVectorizer` in particular becayse it normalized and weights tokens that appear in most documents, so spam or filler words like "lol" would not be counted too much. 

 Because I had no idea how many categories I was making, I decided to start by using the elbow-method to try and find the best number of clusters to use for k-means, which will group my data into however many categories I give it. The elbow method involves creating k-means clusters with a variety of centers (which I pass in as a list), computing the sum of squares for each iteration, and graphing them. From the graph, I identified the "elbow", or the point at which the slope of the line starts decreasing in a linear fashion. This is the best number of clusters for my data. Based on elbowology, my k was set to 9.  

So using k=9 I created a k-means model to label my posts based on their similarity to the 9 centroids generated by the algorithm. I saved the labels generated by this algorithm as a csv file so I could reference it later. I then randomly sampled 10 posts from each of the 9 label groups to get an idea of how k-means categorized them. After I got a vague idea of what kinds of clusters I was imagining (ex: questions about the college in general, posts meant  to generate mass engagement..) I went through 50 random posts and adjusted their labels according to where I thought they fit best.  

## Part 2: Supervised Machine Learning

Now that I had some labels, I hoped to use a supervised machine learning clustering technique to refine my classifications to be closer to what I saw as the categories. Personally analyzing why the clusters were being created the way they were allowed me to better understnad what the computer was doing, and what was happening in my own data, and allowed me to take more control over how the posts were going to be interpreted and ensuring the categories were set up in a way that was actually useful. For example, k-means clustered some posts based on them having similar scentence structures, like "why does". This category on its own is not super useful for the kind of intent analysis I am interested in, but when inspected there was actually a useful trend in the category, since most of the Confessional questions phrased like that were about organizing groups and better understanding social structures at Smith. In my relabeling, I was able to expand this category from the "why does" posts to a more holistic one based on context clues I had about tone and what people were talking about when they mentioned phrases like "CC".

To create a supervised clustering, I used  a k Nearest Neighbors clusterer. First I fit it on a sample of the K means cluster generated labels. My goal was to try and use a theoretically similar clustering to figure out what the best neighbors parameter to use on my hand-curated labels was. I only had the time to label 50 points by hand, so I did not have a complete answer for how the rest of the data was supposed to be classified in that labeling system.  3 neighbors produced the best results on the k-means labels test set, so I used it on the kNN model trained with my hand curated sample as well. I used this to then predict how I would have labeled the rest of the points in my dataset.

I evaluated my kNN model with accuracy scores, and the predictions of the k-means generated test set were far from spectacular (34%). However, given how subjective the data was, how large the tokenized matrix was, and how small the train set was since it takes a long time to hand label, I felt that the accuracy gap was largely to be expected. When I went throught the next 10 points the kNN model predicted based on my labels, my subjective assessment was that I agreed with more of them than the initial k-means labeled. 
 

## Question 3: Value Add Statement
I was having an error with my initital processing of my Confessional text data, I looked up the error and found a  solution (converting my objects into unicode text strings) on stack overflow (0). I also used code from previous labs and homeworks in the creation of my models, especially homework 2, lab 6 and 7, lab 3 and 4, and project 2. 

0. https://stackoverflow.com/questions/39303912/tfidfvectorizer-in-scikit-learn-valueerror-np-nan-is-an-invalid-document
 
